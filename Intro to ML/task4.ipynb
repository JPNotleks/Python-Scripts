{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input, decode_predictions\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.layers import *\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pickle import dump, load\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import neighbors\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization, Input, ReLU\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Conv1D, MaxPooling1D, Add, ZeroPadding1D\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD\n",
    "from keras import regularizers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from keras.utils import to_categorical, plot_model\n",
    "\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from pandas import read_csv, DataFrame\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "#70 minutes for creation of feature images\n",
    "\n",
    "# Seed value (can actually be different for each attribution step)\n",
    "seed_value= 0\n",
    "\n",
    "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set `numpy` pseudo-random generator at a fixed value\n",
    "#import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set `tensorflow` pseudo-random generator at a fixed value\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed_value) # tensorflow 2.x\n",
    "# tf.set_random_seed(seed_value) # tensorflow 1.x\n",
    "\n",
    "\n",
    "\n",
    "#swish activation function\n",
    "class Swish(Activation):\n",
    "    \n",
    "    def __init__(self, activation, **kwargs):\n",
    "        super(Swish, self).__init__(activation, **kwargs)\n",
    "        self.__name__ = 'swish'\n",
    "\n",
    "def swish (x, beta =2.5):\n",
    "    return(x*K.sigmoid(beta*x))\n",
    "get_custom_objects().update({'swish':Swish(swish)})\n",
    "\n",
    "#for setting name in the proper format to fetch image\n",
    "def set_name(name):\n",
    "\tname=str(name)\n",
    "\tif len(name) ==1:\n",
    "\t\tname = '0000' + name\n",
    "\tif len(name) == 2:\n",
    "\t\tname = '000' +name\n",
    "\tif len(name) == 3:\n",
    "\t\tname ='00' + name\n",
    "\tif len(name) == 4:\n",
    "\t\tname = '0' + name\n",
    "\treturn name\n",
    "\n",
    "#fetch numbers of triplets in training set\n",
    "def triplet_extraction(filename = 'train_triplets.txt'):\n",
    "\ttriplets = []\n",
    "\tprint(triplets)\n",
    "\twith open(filename) as reader:\n",
    "\t\tfor line in reader:\n",
    "\t\t\tfirst, second, third = (item.strip() for item in line.split(' ', 2))\n",
    "\t\t\ttriplets.append([int(first), int(second), int(third)])\n",
    "\n",
    "\treturn np.array(triplets)\n",
    "\n",
    "#training set is set up so all predictions are =1, so we create 0s by swapping positions of half of the sample \n",
    "def switch_half(X):\n",
    "\tprint('switching half of data')\n",
    "\tsize = 0.5\n",
    "\tY = np.ones((X.shape[0],))\n",
    "\tindex = rng.random(X.shape[0]) < size\n",
    "\tcontainer = X[index,1]\n",
    "\tX[index,1] = X[index,2]\n",
    "\tX[index,2] = container\n",
    "\n",
    "\tY[index] = 0\n",
    "\treturn X, Y\n",
    "\n",
    "\n",
    "def get_vectors(X):\n",
    "\tprint('getting vectors')\n",
    "\tfinal_X = []\n",
    "\tfeatures = load(open('features_std.pkl', 'rb'))\n",
    "\tfor i in range(0, X.shape[0]):\n",
    "\t\tfinal_X.append([features[X[i,k]] for k in  range (0,3)])\n",
    "\treturn np.array(final_X)\n",
    "\n",
    "#extract images, preprocess them and dump them into file \"features_std.pkl\"\n",
    "def image_extraction():\n",
    "\n",
    "\t#load model\n",
    "\tvgg_model =VGG16()\n",
    "\t#remove the output layer\n",
    "\tvgg_model.layers.pop()\n",
    "\tvgg_model = Model(inputs = vgg_model.inputs, outputs = vgg_model.layers[-1].output)\n",
    "\t#begin with feature extraction\n",
    "\tfeatures = []\n",
    "\tfor i in range(0, 10000):\n",
    "\n",
    "\n",
    "\t\tprint(i)\n",
    "\t\timage = load_img('food/%s.jpg' % set_name(i), target_size =(224,224))\n",
    "\n",
    "\t\timage = img_to_array(image)\n",
    "\t\timage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "\t\timage = preprocess_input(image)\n",
    "\n",
    "\t\tfeature = vgg_model.predict(image)\n",
    "\t\tfeature = feature.reshape((1000,)) #originally 4096\n",
    "\t\tfeatures.append(feature)\n",
    "\tfeatures = np.array(features)\n",
    "\tdump(features, open('features.pkl', 'wb'))\n",
    "\n",
    "\tfeatures = load(open('features.pkl', 'rb'))\n",
    "\tscaler = StandardScaler().fit(features)\n",
    "\tfeatures = scaler.transform(features)\n",
    "\tdump(features, open('features_std.pkl', 'wb'))\n",
    "\n",
    "rng = np.random.default_rng(1)\n",
    "\n",
    "#image_extraction() #can be left out as commentary if the program has already run 1 time at least.\n",
    "X = triplet_extraction() #write the training triplets to X\n",
    "X_test_final = triplet_extraction('test_triplets.txt') #write test triplets to X_test_final\n",
    "X, y = switch_half(X)\n",
    "X_test_final = get_vectors(X_test_final)# write the vectors of the test set\n",
    "\n",
    "active = np.zeros([59544,1])\n",
    "for i in range (0,30):\n",
    "    rng = np.random.default_rng(i)\n",
    "    random.seed(i)\n",
    "    np.random.seed(i)\n",
    "    tf.random.set_seed(i)\n",
    "    mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode = 'max', verbose = 1, save_best_only = True) # for choosing the best model\n",
    "\n",
    "    cvscores = []\n",
    "\n",
    "\n",
    "    train_size = 0.8\n",
    "    index = rng.random(X.shape[0]) < train_size\n",
    "    \n",
    "    X_train = X[index]\n",
    "    X_test = X[~index]\n",
    "\n",
    "    y_train = y[index]\n",
    "    y_test = y[~index]\n",
    "\n",
    "\n",
    "    X_train, X_test = get_vectors(X_train), get_vectors (X_test)\n",
    "\n",
    "    visible = Input(shape=(3, 1000)) #originally 3,4096\n",
    "\n",
    "    #dense = Dense(512)(visible) #i added this\n",
    "    #bat = BatchNormalization()(dense)\n",
    "    #drop = Dropout(0.2)(bat)#i added this\n",
    "    #conv1 = Conv1D(32, kernel_size=1, dilation_rate =1, strides=1, activation='relu', kernel_regularizer=regularizers.l2(0.02))(drop) \n",
    "    #conv2 = Conv1D(32,kernel_size=1, dilation_rate = 1, strides=1,activation='relu', kernel_regularizer=regularizers.l2(0.02))(conv1)\n",
    "\n",
    "    #bat1 = BatchNormalization()(conv2)\n",
    "\n",
    "    #zero1 = ZeroPadding1D(padding=1)(bat1)\n",
    "\n",
    "    #flat = Flatten()(zero1)\n",
    "    #hidden1 = Dense(32, activation='relu')(flat) \n",
    "    #output = Dense(1, activation='sigmoid')(hidden1)\n",
    "    flat = Flatten()(visible)\n",
    "    dense1 = Dense(512)(flat)\n",
    "    bat1 = BatchNormalization()(dense1)\n",
    "    drop1 = Dropout(0.35)(bat1)\n",
    "    dense2 = Dense(512,activation = 'relu')(drop1)\n",
    "    dense3 = Dense(256)(dense2)\n",
    "    bat2 = BatchNormalization()(dense3)\n",
    "    drop2 = Dropout(0.2)(bat2)\n",
    "    dense4 = Dense(256, activation = 'relu')(drop2)\n",
    "    output = Dense(1, activation = 'sigmoid')(dense4)\n",
    "    \n",
    "\n",
    "   \n",
    "\n",
    "    model =Model(inputs = visible, outputs = output)\n",
    "\n",
    "    opt = SGD(lr=0.001, momentum=0.9)\n",
    "    model.compile( optimizer = 'adam',loss ='binary_crossentropy',  metrics =['accuracy'])\n",
    " \n",
    "\n",
    "    history = model.fit(X_train, y_train, epochs = 40, batch_size = 200, validation_data =(X_test, y_test), callbacks =[mc]) # og epochs =40,batch =200\n",
    "    saved_model =load_model('best_model.h5', custom_objects = {'Swish': Swish(swish)})\n",
    "    scores = saved_model.evaluate(X_test, y_test)\n",
    "    \n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "    cvscores.append(scores[1] * 100)\n",
    "\n",
    "    print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))\n",
    "\n",
    "    model.summary()\n",
    "  \n",
    "    active = np.add(active,np.array(saved_model.predict(X_test_final)).reshape(59544,1))\n",
    "\n",
    "active = active/30\n",
    "\n",
    "\n",
    "\n",
    "ones = np.where(active > 0.5)\n",
    "df = DataFrame(active)\n",
    "df.loc[:] = 0\n",
    "df.loc[ones] = 1\n",
    "df.to_csv('prediction_task4_16.csv', index=False, header = False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
