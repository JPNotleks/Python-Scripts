{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imputation finished\n",
      "imputation finished\n",
      "0.8518662806001579\n",
      "score at 1\n",
      "0.9427217688865491\n",
      "score at 2\n",
      "PASS\n",
      "PASS\n",
      "PASS\n",
      "(12664,)\n",
      "(12664, 10)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Apr 20 21:05:52 2021\n",
    "\n",
    "@author: ELM21\n",
    "\"\"\"\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "import operator\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "test = pd.read_csv('test_features.csv')\n",
    "features = pd.read_csv('train_features.csv') #all features from csv\n",
    "labels = pd.read_csv('train_labels.csv') # labels for all subtasks\n",
    "task1labels = list(labels)[1:11] #labels for subtasks 1,2,3\n",
    "task2labels = ['LABEL_Sepsis']\n",
    "task3labels = list(labels)[12:]\n",
    "allnames = list(features)\n",
    "names = list(features)[3:]\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "#\"crunch dataset\" takes dataset and get, min for each patient each entry, variance and mean\n",
    "def transformdata(data):\n",
    "\n",
    "\tA = data.groupby(np.arange(len(data))//12).min()\n",
    "\tB = data.groupby(np.arange(len(data))//12).max()\n",
    "\tC = data.groupby(np.arange(len(data))//12).var()\n",
    "\tD = data.groupby(np.arange(len(data))//12).mean()\n",
    "\tF = data.groupby(np.arange(len(data))//12).last()\n",
    "\n",
    "\tanames = ['min of ' + s for s in names]\n",
    "\tbnames = ['max of ' + s for s in names]\n",
    "\tcnames = ['var of ' + s for s in names]\n",
    "\tdnames = ['pid','Time','Age']+['mean of ' + s for s in names]\n",
    "\tfnames = ['last measurement of ' +s for s in names]\n",
    "\n",
    "\n",
    "\tA = A.drop(['pid','Time','Age'],axis =1)\n",
    "\tB = B.drop(['pid','Time','Age'],axis =1)\n",
    "\tC = C.drop(['pid','Time','Age'],axis =1)\n",
    "\tF = F.drop(['pid','Time','Age'],axis =1)\n",
    "\n",
    "\n",
    "\tA.columns=anames\n",
    "\tB.columns=bnames\n",
    "\tC.columns=cnames\n",
    "\tD.columns = dnames\n",
    "\tF.columns = fnames\n",
    "\n",
    "\tE = pd.concat([D,F,A,B,C],axis=1)\n",
    "\tE = E.drop(['pid','Time'],axis=1)\n",
    "\n",
    "\t#E.interpolate(method = \"akima\", inplace= False)\n",
    "\timputer = SimpleImputer(missing_values=np.nan,strategy = 'mean')\n",
    "\n",
    "\timputer.fit(E)\n",
    "\tE = imputer.transform(E)\n",
    "\n",
    "\tprint(\"imputation finished\")\n",
    "    \n",
    "\treturn E\n",
    "\n",
    "\n",
    "def predict1(fulldataset,fulltestset):\n",
    "           \n",
    "   \n",
    "    Y = labels[task1labels]\n",
    "    \n",
    "    \n",
    "    #fulldataset = scaler.fit_transform(fulldataset)\n",
    "    #fulltestset = scaler.transform(fulltestset)\n",
    "    \n",
    "    rfc = RandomForestClassifier(n_estimators=1000,bootstrap=True,max_features = 'sqrt',oob_score=True,n_jobs=-1,random_state=1, class_weight = 'balanced_subsample')\n",
    "    rfc.fit(fulldataset,Y)\n",
    "    \n",
    "    print(rfc.oob_score_)\n",
    "    print('score at 1')\n",
    "    \n",
    "\n",
    "    y1=rfc.predict_proba(fulltestset)\n",
    "    return y1\n",
    "        \n",
    "def predict2(fulldataset,fulltestset):\n",
    "           \n",
    "   \n",
    "      \n",
    "    Y = labels[task2labels]\n",
    "    #fulldataset = scaler.fit_transform(fulldataset)\n",
    "    #fulltestset = scaler.transform(fulltestset)\n",
    "\n",
    "\n",
    "    rfc = RandomForestClassifier(n_estimators=1000,bootstrap=True,max_features = 'sqrt',oob_score=True,n_jobs=-1,random_state=1, class_weight=\"balanced_subsample\")\n",
    "    rfc.fit(fulldataset,np.ravel(Y))\n",
    "    \n",
    "    print(rfc.oob_score_)\n",
    "    print('score at 2')\n",
    "    \n",
    "    y2=rfc.predict_proba(fulltestset)\n",
    "    return y2\n",
    "\n",
    "def predict3(fulldataset,fulltestset):\n",
    "    \n",
    "    Y=labels[task3labels]\n",
    "    \n",
    "    fulldataset = scaler.fit_transform(fulldataset)\n",
    "    fulltestset = scaler.transform(fulltestset)\n",
    "\n",
    "    lasso_params = {'alpha' : np.linspace(0.01,0.03, 4)}\n",
    "    ridge_params = {'alpha': np.linspace(200, 500, 4)}\n",
    "\n",
    "    models = {'OLS': linear_model.LinearRegression(),\n",
    "    \t\t'Lasso': GridSearchCV(linear_model.Lasso(), param_grid = lasso_params).fit(fulldataset, Y).best_estimator_,\n",
    "    \t\t'Ridge': GridSearchCV(linear_model.Ridge(), param_grid = ridge_params).fit(fulldataset, Y).best_estimator_}\n",
    "    \n",
    "    scores = {}\n",
    "    for model in models:\n",
    "      print(\"PASS\")\n",
    "      score = np.mean(cross_val_score(models[model], fulldataset, Y, scoring='r2', cv = 3))\n",
    "      scores[model] = score\n",
    "    final_model = max(scores.items(), key = operator.itemgetter(1))[0]\n",
    "\n",
    "    md = models[final_model]\n",
    "\n",
    "    y3 = md.predict(fulltestset)\n",
    "\n",
    "    return y3\n",
    "\n",
    "def getids(dataset):\n",
    "    pid = dataset['pid']\n",
    "    pid = pid.groupby(np.arange(len(dataset))//12).mean()\n",
    "\n",
    "    return pid\n",
    "\n",
    "def getpredictions(dataset,testset):\n",
    "    \n",
    "\n",
    "    X = transformdata(dataset)\n",
    "    y = transformdata(testset)\n",
    "\n",
    "    y1 = predict1(X,y)\n",
    "    y2 = predict2(X,y)\n",
    "    y3 = predict3(X,y)\n",
    "\n",
    "    y1 = np.array(y1)\n",
    "    y1 = y1[:,:,1]\n",
    "    y1 = np.transpose(y1)\n",
    "    y2 = y2[:,1]\n",
    "\n",
    "    print(np.shape(np.array(y2)))\n",
    "    print(np.shape(np.array(y1)))\n",
    "    \n",
    "    yall = np.column_stack((y1,y2,y3))\n",
    "    ids = getids(testset)\n",
    "    yall = np.column_stack((ids,yall))\n",
    "    yall =pd.DataFrame(yall, columns = list(labels))\n",
    "\n",
    "    yall.to_csv('predictions.csv',index=False,float_format = '%.3f')\n",
    "    \n",
    "getpredictions(features,test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
